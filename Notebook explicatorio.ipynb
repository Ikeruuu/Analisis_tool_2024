{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Logo de AA1](Logo.png) \n",
    "# Documentación Python\n",
    "Hemos creado un proyecto basado en la detección de palabras clave en redes sociales como Twitter o Telegram, tratando de analizar si puede tener un impacto como moneda en páginas `DeFi`, y todo a base del `Machine Learning`. De esta manera, relacionamos el trading de `criptomonedas` con las `redes sociales`, algo innovador y que a día de hoy aún no tiene una solución exacta. \n",
    "\n",
    "Para ello lo que hemos usado principalmente es código de Python, 2 APIs (una de Telegram y otra de Twitter), 1 bot de Telegram y Airflow, para automatizar y optimizar el proceso de ejecución mediante un Docker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previo\n",
    "\n",
    "Antes de nada vamos a hablar de todos los imports de bibliotecas que tenemos que hacer para empezar.\n",
    "\n",
    "Yendo de arriba abajo, empecemos:\n",
    "\n",
    "1. `sqlite3` es la base de datos dinámica\n",
    "2. `tweepy` APi de twitter\n",
    "3. `pandas` una biblioteca para manipulación y análisis de datos\n",
    "4. `datetime` maneja fechas y horas\n",
    "5. `collections` permite contar elementos en una lista o colección de datos.\n",
    "6. `re` biblioteca para trabajar con expresiones regulares\n",
    "7. `RandomForestClassifier` clasificador de Machine Learninng elegido\n",
    "8. `SentimentIntensityAnalyzer` herramienta para análisis de sentimiento basada en texto\n",
    "9. `bot` para importar el bot que se usará para mandar mensajes\n",
    "10. `TelegramClient` API de Telegram\n",
    "11. `smtplib` proporciona herramientas para enviar correos electrónicos mediante el protocolo SMTP\n",
    "12. `MIMEText` permite crear el contenido del correo electrónico en formato texto\n",
    "13. `asyncio` se usa para definir funciones asíncronas como las de Telegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from telegram import Bot\n",
    "from telethon import TelegramClient\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "import asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creación base de datos dinámica\n",
    "\n",
    "Esta primera parte se encarga como dije antes de crear una base de datos SQLite en la que se guardan las palabras clave que vamos filtrando. Con el 3º comando se ejecuta siguiendo las instrucciones en rojo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. CONFIGURACIÓN DE BASE DE DATOS\n",
    "# ----------------------------\n",
    "\n",
    "# Conexión SQLite\n",
    "conn = sqlite3.connect('crypto_trends.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Crear tabla para palabras clave\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS keywords (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        keyword TEXT UNIQUE,\n",
    "        last_checked TIMESTAMP,\n",
    "        frequency INTEGER,\n",
    "        sentiment REAL\n",
    "    )\n",
    "''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conectar las APIs con nustro código\n",
    "\n",
    "Aquí conectamos las API que utilizaremos para la recogida de mensajes y posteriormente, filtrar las `Keyword`. \n",
    "\n",
    "Por un lado tenemos `Tweepy` para Twitter: para usarla debemos de crear una cuenta de Twitter creator y crear un proyecto. En ese momento te darán un Key y una clave secret. Tendrás que solicitar un Token y a su vez, te darán otra clave secret para ese Token. Una vez sabiendo todo eso, se pone donde pusimos los mensajes entre <> y listo.\n",
    "\n",
    "Para la parte de `Telegram`, es parecido, necesitas tu Id y una clave Hash. Estas se consiguen yendo a las herramientas de la API y haciendo un form para pedirlas. Importante recordar que las funciones a implementar en Telegram son asíncronas, como ya mencioné antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. CONEXIÓN A APIs\n",
    "# ----------------------------\n",
    "\n",
    "API_KEY = 'your_api_key'\n",
    "API_SECRET = 'your_api_secret'\n",
    "ACCESS_TOKEN = 'your_access_token'\n",
    "ACCESS_TOKEN_SECRET = 'your_access_token_secret'\n",
    "\n",
    "# Autenticación de Twitter con Tweepy\n",
    "auth = tweepy.OAuthHandler(API_KEY, API_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def fetch_tweets(keyword, max_tweets=100):\n",
    "    \"\"\"Captura tweets desde Twitter API\"\"\"\n",
    "    tweets_data = []\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, q=keyword, lang='en', tweet_mode='extended').items(max_tweets):\n",
    "        tweets_data.append(tweet.full_text)\n",
    "    return tweets_data\n",
    "\n",
    "\n",
    "# Configuración de la API de Telegram\n",
    "api_id = 123456\n",
    "api_hash = \"abcdef1234567890abcdef1234567890\"\n",
    "\n",
    "async def fetch_messages(chat, limit=10):\n",
    "    async with TelegramClient(\"my_session\", api_id, api_hash) as client:\n",
    "        messages = await client.get_messages(chat, limit=limit)\n",
    "        return [msg.message for msg in messages if msg.message]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Análisis de sentimiento (subjetividad)\n",
    "\n",
    "En esta parte mediante la biblioteca implmementada podremos ver el `sentimiento` promedio de un texto/palabra, sacando así un número para usar como parámetro posteriormente al clasificar las palabras en clave o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. ANÁLISIS DE SENTIMIENTO\n",
    "# ----------------------------\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(texts):\n",
    "    \"\"\"Calcula el sentimiento promedio usando VADER.\"\"\"\n",
    "    scores = [analyzer.polarity_scores(text)['compound'] for text in texts]\n",
    "    return sum(scores) / len(scores) if scores else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtro de palabras clave\n",
    "\n",
    "Aquí simplemente lo que se hace es coger los textos sacados mediante las APIs y ver que palabras se repiten y cuántas veces, llegando a un tope que lo que hace es meterlas a una lista como palabras relevantes. El número se puede modificar en el `most_common` según cómo de estricto quieres ser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. FILTRO DE PALABRAS CLAVE\n",
    "# ----------------------------\n",
    "\n",
    "def extract_keywords(texts, existing_keywords):\n",
    "    \"\"\"Extrae palabras clave nuevas.\"\"\"\n",
    "    all_words = ' '.join(texts).lower()\n",
    "    words = re.findall(r'\\b[a-zA-Z]+\\b', all_words)\n",
    "    common_words = Counter(words).most_common(20)\n",
    "    new_keywords = [word for word, count in common_words if word not in existing_keywords]\n",
    "    return new_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entrenamiento del modelo\n",
    "\n",
    "Lo primero de todo es la base de datos que puesto de ejemplo con la frecuencia de palabras, su sentimiento y si es exitosa o no, sin embargo, se podría implementar otra `base de datos real` en la que se guardara información sobre criptos exitosas, o palabras que hayan podido tener un success.\n",
    "\n",
    "Después de eso, procedemos a entrenar el modelo, en este caso usando random forest, pero se podrían usar otros modelos de clasificación que quizás sean más eficientes. Es importante entrenar el modelo con datos reales y de criptos para que se acostumbre a ese entorno, y que no sufra de Overfitting o Underfitting. En caso de haber problema, siempre está la opción de hacer balance de pesoss entre entreno y test, para que el modelo sea mejor. (X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1234, stratify=y))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. ENTRENAMIENTO DEL MODELO\n",
    "# ----------------------------\n",
    "\n",
    "# Datos históricos simulados\n",
    "training_data = pd.DataFrame({\n",
    "    'frequency': [100, 200, 1500, 800, 50],\n",
    "    'sentiment': [0.8, 0.6, 0.7, 0.9, 0.4],\n",
    "    'success': [1, 1, 1, 1, 0]\n",
    "})\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "X_train = training_data[['frequency', 'sentiment']]\n",
    "y_train = training_data['success']\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Guardado y análisis de palabras clave\n",
    "\n",
    "En este apartado lo que se hace es guardar y actualizar palabras clave previamente filtradas, usando las funciones ya definidas sobre el sentimiento, frecuencia y predicción mediante el modelo entrenado. Por último, se establece que si la predicción es buena, entoncess la palabra será exitosa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 6. GUARDADO Y ANÁLISIS DE PALABRAS CLAVE\n",
    "# ----------------------------\n",
    "\n",
    "def save_keyword(keyword, frequency, sentiment):\n",
    "    \"\"\"Guarda o actualiza palabras clave en la base de datos.\"\"\"\n",
    "    now = datetime.now()\n",
    "    cursor.execute('''\n",
    "        INSERT INTO keywords (keyword, last_checked, frequency, sentiment)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "        ON CONFLICT(keyword) DO UPDATE SET \n",
    "            last_checked=excluded.last_checked,\n",
    "            frequency=excluded.frequency,\n",
    "            sentiment=excluded.sentiment\n",
    "    ''', (keyword, now, frequency, sentiment))\n",
    "    conn.commit()\n",
    "\n",
    "def analyze_keyword(keyword, max_tweets=100):\n",
    "    \"\"\"Analiza una palabra clave con frecuencia, sentimiento y predicción.\"\"\"\n",
    "    tweets = fetch_tweets(keyword, max_tweets)\n",
    "    frequency = len(tweets)\n",
    "    sentiment = analyze_sentiment(tweets)\n",
    "    prediction = model.predict([[frequency, sentiment]])[0]\n",
    "    save_keyword(keyword, frequency, sentiment)\n",
    "    return {\n",
    "        'keyword': keyword,\n",
    "        'frequency': frequency,\n",
    "        'sentiment': sentiment,\n",
    "        'prediction': 'Exitoso' if prediction == 1 else 'No exitoso'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Notificaciones a correo y Telegram\n",
    "\n",
    "Aquí lo que hay que hacer es crear un bot en Telegram mediante BotFather, pedir su ID de token y meterlo en neustro código, para así configurarse como un bot que se dedica a mandar notificaciones según los requisitos del Pipeline. \n",
    "\n",
    "Lo mismo con las notificaciones Mail pero sin un bot, en este caso usamos la función MIMEText, y así solo necesitaremos nuestro correo y contraseña (del que la envía). Si no quieres que se vean se pueden meter esas claves en un archivo .env y traerlas desde allí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 7. NOTIFICACIONES\n",
    "# ----------------------------\n",
    "# Función asincrónica que envía el mensaje\n",
    "async def send_telegram_update(chat_id, message):\n",
    "    \"\"\"Envía un mensaje a través de Telegram utilizando async.\"\"\"\n",
    "    token = 'tu_bot_token_aqui'  # Reemplaza con tu token de bot\n",
    "    bot = Bot(token=token)\n",
    "    \n",
    "    try:\n",
    "        # Usamos 'await' para llamar a la función asincrónica\n",
    "        await bot.send_message(chat_id=chat_id, text=message)\n",
    "        print(\"Mensaje enviado con éxito.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al enviar el mensaje: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def send_email_notification(subject, message, recipient_email):\n",
    "    \"\"\"Envía una notificación por correo electrónico.\"\"\"\n",
    "    sender_email = \"your_email@gmail.com\"\n",
    "    sender_password = \"your_email_password\"\n",
    "\n",
    "    msg = MIMEText(message)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = sender_email\n",
    "    msg['To'] = recipient_email\n",
    "\n",
    "    with smtplib.SMTP_SSL('smtp.gmail.com', 465) as server:\n",
    "        server.login(sender_email, sender_password)\n",
    "        server.sendmail(sender_email, recipient_email, msg.as_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pipeline que conjunta todo\n",
    "\n",
    "La última parte del código simplemente junta todo lo que hemos estado usando, estableciendo unos parámetros para enviar lsa notificaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 8. PIPELINE COMPLETO\n",
    "# ----------------------------\n",
    "\n",
    "def process_pipeline(max_tweets=100):\n",
    "    \"\"\"Pipeline completo de detección y análisis.\"\"\"\n",
    "    # Obtener palabras clave existentes\n",
    "    cursor.execute('SELECT keyword FROM keywords')\n",
    "    existing_keywords = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "    # Captar textos\n",
    "    tweets = fetch_tweets('crypto', max_tweets)\n",
    "    new_keywords = extract_keywords(tweets, existing_keywords)\n",
    "\n",
    "    # Analizar palabras clave nuevas\n",
    "    for keyword in new_keywords:\n",
    "        analysis = analyze_keyword(keyword)\n",
    "        print(f\"Análisis para {keyword}: {analysis}\")\n",
    "\n",
    "        # Notificar si es relevante\n",
    "        if analysis['frequency'] > 100 and analysis['sentiment'] > 0.8:\n",
    "            \n",
    "            # Función principal asincrónica\n",
    "            async def main():\n",
    "                chat_id = '123456789'  # Reemplaza con el chat_id correcto\n",
    "                message = f\"La palabra '{keyword}' muestra alta relevancia.\\nSentimiento: {analysis['sentiment']}\\nFrecuencia: {analysis['frequency']}\"\n",
    "                await send_telegram_update(chat_id, message)\n",
    "            asyncio.run(main())\n",
    "\n",
    "            send_email_notification(\n",
    "                subject=f\"Tendencia detectada: {keyword}\",\n",
    "                message=f\"La palabra '{keyword}' muestra alta relevancia.\\nSentimiento: {analysis['sentiment']}\\nFrecuencia: {analysis['frequency']}\",\n",
    "                recipient_email='recipient@example.com'\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline\n",
    "process_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG\n",
    "\n",
    "El otro código que tenemos es el del DAG que importa el archivo de Python para usarlo en el Airflow posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "from pred_crypto_memes import process_pipeline\n",
    "\n",
    "with DAG('crypto_monitoring', start_date=datetime(2024, 1, 1), schedule_interval='@hourly') as dag:\n",
    "    monitor_task = PythonOperator(\n",
    "        task_id='monitor_social_media',\n",
    "        python_callable=process_pipeline\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración airflow en Docker\n",
    "\n",
    "Esto es el archivo yaml., que se importa en el Docker para crear los containers y ejecutar el código automáticamente. (Salen como si fuesen errores porque al no ser un archivo .py, el Python no lo detecta.)\n",
    "\n",
    "Los comandos del 2º código simplemente lo que hacen es instalar el Airflow, crearnos un usuario para usar los logs del webserver, y dentro del Docker, iniciar los container con el últmo de todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyyaml\n",
    "import yaml\n",
    "---\n",
    "version: \"3\"\n",
    "x-airflow-common:\n",
    "  &airflow-common\n",
    "  \n",
    "  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.3}\n",
    "  # build: .\n",
    "  environment:\n",
    "    &airflow-common-env\n",
    "    AIRFLOW__CORE__EXECUTOR: CeleryExecutor\n",
    "    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://airflow:airflow@postgres/airflow\n",
    "    AIRFLOW__CELERY__BROKER_URL: redis://:@redis:6379/0\n",
    "    AIRFLOW__CORE__FERNET_KEY: ''\n",
    "    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'\n",
    "    AIRFLOW__CORE__LOAD_EXAMPLES: 'true'\n",
    "    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'\n",
    "   \n",
    "    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'\n",
    "    \n",
    "    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}\n",
    "    \n",
    "  volumes:\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n",
    "    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n",
    "  user: \"${AIRFLOW_UID:-50000}:0\" \n",
    "  depends_on:\n",
    "    &airflow-common-depends-on\n",
    "    redis:\n",
    "      condition: service_healthy\n",
    "    postgres:\n",
    "      condition: service_healthy\n",
    "\n",
    "services:\n",
    "  postgres:\n",
    "    image: postgres:13\n",
    "    environment:\n",
    "      POSTGRES_USER: airflow\n",
    "      POSTGRES_PASSWORD: airflow\n",
    "      POSTGRES_DB: airflow\n",
    "    volumes:\n",
    "      - postgres-db-volume:/var/lib/postgresql/data\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]\n",
    "      interval: 5s\n",
    "      retries: 5\n",
    "      start_period: 5s\n",
    "    restart: always\n",
    "\n",
    "  redis:\n",
    "    \n",
    "    image: redis:7.2-bookworm\n",
    "    expose:\n",
    "      - 6379\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"ping\"]\n",
    "      interval: 10s\n",
    "      timeout: 30s\n",
    "      retries: 50\n",
    "      start_period: 30s\n",
    "    restart: always\n",
    "\n",
    "  airflow-webserver:\n",
    "    <<: *airflow-common\n",
    "    command: webserver\n",
    "    ports:\n",
    "      - \"8080:8080\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "  \n",
    "\n",
    "  airflow-scheduler:\n",
    "    <<: *airflow-common\n",
    "    command: scheduler\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8974/health\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "\n",
    "\n",
    "  airflow-worker:\n",
    "    <<: *airflow-common\n",
    "    command: celery worker\n",
    "    healthcheck:\n",
    "      # yamllint disable rule:line-length\n",
    "      test:\n",
    "        - \"CMD-SHELL\"\n",
    "        - 'celery --app airflow.providers.celery.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\" || celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@$${HOSTNAME}\"'\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "\n",
    "  airflow-triggerer:\n",
    "    <<: *airflow-common\n",
    "    command: triggerer\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"$${HOSTNAME}\"']\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "\n",
    "  airflow-init:\n",
    "    <<: *airflow-common\n",
    "    # yamllint disable rule:line-length\n",
    "    command: version\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      _AIRFLOW_DB_MIGRATE: 'true'\n",
    "      _AIRFLOW_WWW_USER_CREATE: 'true'\n",
    "      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n",
    "      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}\n",
    "\n",
    "  airflow-cli:\n",
    "    <<: *airflow-common\n",
    "    profiles:\n",
    "      - debug\n",
    "    environment:\n",
    "      <<: *airflow-common-env\n",
    "      CONNECTION_CHECK_MAX_COUNT: \"0\"\n",
    "    \n",
    "    command:\n",
    "      - bash\n",
    "      - -c\n",
    "      - airflow\n",
    "\n",
    "  \n",
    "  flower:\n",
    "    <<: *airflow-common\n",
    "    command: celery flower\n",
    "    profiles:\n",
    "      - flower\n",
    "    ports:\n",
    "      - \"5555:5555\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:5555/\"]\n",
    "      interval: 10s\n",
    "      timeout: 10s\n",
    "      retries: 5\n",
    "    restart: always\n",
    "\n",
    "volumes:\n",
    "  postgres-db-volume:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv airflow-env #crear entorno env\n",
    "source airflow-env/bin/activate  # Linux/MacOS\n",
    ".\\airflow-env\\Scripts\\activate   # Windows\n",
    "\n",
    "pip install apache-airflow #instalar airflow\n",
    "\n",
    "airflow db init #iniciar base de datos\n",
    "\n",
    "airflow users create \\ #crear uusario\n",
    "    --username admin \\\n",
    "    --firstname Admin \\\n",
    "    --lastname User \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.com \\\n",
    "    --password admin\n",
    "\n",
    "airflow webserver -p 8080  # Inicia la interfaz gráfica\n",
    "airflow scheduler          # Inicia el scheduler\n",
    "\n",
    "curl -LfO 'https://airflow.apache.org/docs/apache-airflow/stable/docker-compose.yaml' #enlace a archivo yaml adjuntado antes\n",
    "\n",
    "docker-compose up airflow-init # iniciar Airflow\n",
    "\n",
    "docker-compose up #iniciar los servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anexo: Explicación preprocesado bases de datos\n",
    "\n",
    "En caso de que las bases de datos tengan diferentes problemas de tipografía o datos vacíos (NaN), pues aquí se podría ver un ejemplo de cómo modificar esos detalles, además de poner cabeceras, separar los datos en 2 partes o preprocesar los datos en números, si es que son str (datos categóricos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikit.preprocessing import LabelEncoder\n",
    "\n",
    "cabecera = ['atr'+str(x) for x in range(1,25)] # for en línea \n",
    "\n",
    "#para crear elementos de una lista\n",
    "cabecera.append('clase') # la clase está en la última posición\n",
    "\n",
    "datos = pd.read_csv('datos.data', names=cabecera, sep=',', na_values='?')\n",
    "filas, columnas = datos.shape\n",
    "#eliminar datos desconocidos\n",
    "\n",
    "datos=datos.dropna()\n",
    "\n",
    "\n",
    "# la clase está en la última columna \n",
    "# separamos los atributos y los almacenamos en X\n",
    "X = datos.drop(['clase'], axis=1)\n",
    "print('\\n Atributos')\n",
    "print(X)\n",
    "\n",
    "# Como la clase toma dos valores diferentes, transformamos la clase con un Label Encoder\n",
    "\n",
    "class_enc = preprocessing.LabelEncoder()\n",
    "datos['clase'] = class_enc.fit_transform(datos['clase'])\n",
    "\n",
    "# separamos la clase y la almacenamos en Y\n",
    "y = datos['clase']\n",
    "\n",
    "print('\\n Clase ')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras potenciales a futuro y diversos cambios a hacer\n",
    "\n",
    "- En los filtros de los tweets que se recogen, se podría restringir a cuentas de X seguidores o que sean verificados, para captar grandes influencias. \n",
    "\n",
    "- Otra es poner filtros de letras como un $, que es el carácter que se usa al hablar de criptos, y podría ser interesantes.\n",
    "\n",
    "- Realizar búsquedas en otras redes sociales que también son influyentes.\n",
    "\n",
    "- Probar otros métodos de entrenamiento ya sea de `bagging` o `boosting`, incluso redes neuronales, aunque es más complejo.\n",
    "\n",
    "- Mejorar el código yaml. para modificar los tiempos y que sea más eficiente.\n",
    "\n",
    "- Filtrar las claves de usadas de los Tokens o de las API en archivos .env para que no puedan ser robadas."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
